# -*- coding: utf-8 -*-
"""Asia Air Quality 2023 EDA & Forecasting by Cities

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/asia-air-quality-2023-eda-forecasting-by-cities-cfc1f8a1-7610-442b-a80e-c9776977709f.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20250304/auto/storage/goog4_request%26X-Goog-Date%3D20250304T020720Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D17b279dfe2b411a4fb5c9a90ebd6b7411870975db9997ad43cb24fab960d9640d0e0fb22bea4c2db94c52fc3c97e96554a986c034446d6e925917b7ecadfe3e67151a4ccb1e13b806ec045cccbc73c20563ab4a1a6890fb997297cbdd4f2070e669777c5c6fb5d7322a5d91e1238e90f3c7f9683d9d74d5a44825b5c52a4c08a7de286b5d38696ec3ead82c974bae32ef7287cd86d70b701614c61e62f3c9725650753d74e0d8a767aa8a3033e94c3e4e5a9ffd59f8d342515992a993f1729906aa7fd1eea25e5c94e6d30460d0b2a287597da7cd528582cc3e5d52e85134b2031041245068dfde78f3a9bd3706cc0e264ed04ee2d53234e7c65d556f1eb1d6d
"""

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.
import kagglehub
shruthiiiee_asia_2023_air_report_2000_cities_path = kagglehub.dataset_download('shruthiiiee/asia-2023-air-report-2000-cities')

print('Data source import complete.')

"""<a id='goal'></a>
<p style="font-size: 18px; font-family:arial; text-align: justify; border-bottom: 2px solid #c09f80;">Goal</p>
<p style="font-size: 16px; font-family:arial; text-align: justify;">
The goal of this project is to analyze air pollution levels in various Asian cities, detect trends and patterns, and utilize historical data for forecasting monthly pollution levels in the future.</p>

<p style="font-size: 18px; font-family:arial; text-align: justify; border-bottom: 2px solid #c09f80;">
Dataset Descriptionüí°</p>
<p style='font-size: 16px; font-family:arial; text-align: justify;'>The <a href="https://www.kaggle.com/datasets/shruthiiiee/asia-2023-air-report-2000-cities">dataset</a> contains information on Air pollution index and rankings for over 2164 cities across Asia in 2023.
</p>

<ul style='font-family: arial; font-size: 16px;'><b><u>Features and Descriptions</u></b>
    <li>Rank: The ranking of cities based on their annual average PM2.5 concentration (Œºg/m¬≥).</li>
    <li>City: The name of the city.</li>
    <li>2023: The average pollution index for the year 2023.</li>
    <li>Country: The corresponding country for each city.</li>
    <li>Jan, Feb, Mar, Apr, May, Jun, Jul, Aug, Sep, Oct, Nov, Dec: Monthly pollution levels from January to December.</li>
</ul>

<p style="font-size: 18px; font-family:arial; text-align: justify; border-bottom: 2px solid #c09f80;">
Acknowedgementüí°</p>

<p style='font-size: 16px; font-family:arial; text-align: justify;'>
A heartfelt shoutout to <a href=https://www.kaggle.com/shruthiiiee>Shruthi</a>, the diligent dataset owner. Your hard work is invaluable!.</p>

<left>
    <p style="font-size: 18px; font-family:arial; text-align: justify; border-bottom: 2px solid #c09f80;"><b>
        Table of Contents</b></p>
    <ol id='contents' style='font-family: arial; font-size: 16px'>
    <li><a href='#goal'>Goal</a></li>
    <li><a href='#library'>Libraries &amp Dataset</a></li>
    <li><a href='#info'>Basic Information &amp Cleaning</a></li>
    <li><a href='#eda'>EDA</a></li>
    <li><a href='#preprocessing'>Preprocessing</a></li>
    <li><a href='#train-test'>Train &amp Test</a></li>
    <li><a href='#model'>Time Series Model</a></li>
</ol>

<a id='library'></a>
<h2 style=" font-family:arial; text-align: justify; border-bottom: 2px solid #c09f80;">Libraries &amp Dataset</b><a href='#contents'> üìñ</a></h2>
"""

pip install pmdarima --quiet

import os, time, warnings
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
from scipy import stats
from scipy.stats.distributions import chi2

import statsmodels.graphics.tsaplots as sgt
import statsmodels.tsa.stattools as sts
from statsmodels.tsa.arima_model import ARIMA
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from pmdarima import auto_arima

import shap

## Set seed, style and source url
from google.colab import drive
drive.mount('/content/drive')

SEED = 91
plt.style.use('fast')

SOURCE = '/content/drive/My Drive/DATASET/AirQuality.csv'

"""<a id='info'></a>
<h2 style="font-family:arial; text-align: justify; border-bottom: 2px solid #c09f80;">Basic Information &amp Cleaning<a href='#contents'> üîç</a></h2>
"""

## load data & show samples
og_df = pd.read_csv(SOURCE)
og_df.head(5)
df = og_df.copy()

## basic info showing data types, missing values
df.info()

"""> All monthly pollution level features have inappropriate data types. From the sample, they should be float not object."""

## convert monthly pollution to float and pass in np.nan when errors are raised
for col in df.columns[-12:]:
    df[col] = pd.to_numeric(df[col],errors="coerce")

## check missing values by columns
df.isnull().sum(axis=0).sort_values(ascending=False)

"""> Top 3 columns with missing values are Nov, Jan, Feb"""

## check missing values by columns
np.sum(df.isnull().sum(axis=1) > 0)

"""> 270 out of 2164 rows (12.48%) contain at least one missing value."""

## fill in missing values
## we will employ linear interpolate (both direction) to fill in missing value using the valid adjacent data points
df.iloc[:,-12:] = df.iloc[:,-12:].interpolate(method='linear',axis=1,limit_direction='both')

## total number of missing values
sum(df.isnull().sum())

df.head(3)

## check number of duplicate rows
df.drop("Rank",axis=1).duplicated(keep='first').sum()

## descriptive statistics
df.describe()

## split to categorical table and numerical table
num_col = df.select_dtypes(exclude='object')
cat_col = df.select_dtypes(include='object')
cat_col["2023"] = df["2023"]
print(num_col.shape)
print(cat_col.shape)

## trying to identify 0 and inf
num_col.describe().loc[["min","max"],:]

"""> There are zeros in some columns. No negative nor infinity."""

## identify unique values and how frequent they appear
cat_col.iloc[:,:-1].describe()

"""> Some cities appear more than once. e.g., Miyoshi appears 3 times. And all of them are from the same country, Japan."""

## show the rows that has the same City and Country
df[df[['City','Country']].duplicated(keep=False)].sort_values("City")

## drop the rows that has the same City and Country (don't know how to distinguish them)
df.drop_duplicates(subset=['City','Country'],keep='first',inplace=True)
df.shape

## show the same city names, different countries
df[df['City'].duplicated(keep=False)].sort_values(by='City',ascending=True)

## drop index 1417 Country == China, China
df.drop(1417,axis=0,inplace=True)

"""> We will go through Country to reach City when training the model!

<a id='eda'></a>
<h2 style="font-family:arial; text-align: justify; border-bottom: 2px solid #c09f80;">EDA <a href='#contents'>üìà</a></h2>
"""

map_df = og_df[["Country","2023"]].groupby("Country").agg("mean").reset_index()
px.choropleth(data_frame = map_df,
              locationmode='country names',locations='Country',
              title='Average Pollution Index (2023) by COUNTRIES',color='2023',
              projection='equirectangular',scope='asia', color_continuous_scale="Reds")

"""Top 5<br>
'Bangladesh', 'Pakistan', 'Tajikistan', 'India','United Arab Emirates'<br>
66.1, 59.57, 48.075, 47.614, 44.6

Botton 5<br>
'Hong Kong SAR', 'Taiwan', 'Philippines', 'Singapore', 'Japan'<br>
16.67, 14.43, 14.2, 13.4, 9.287
"""

px.line(df.iloc[:5,-12:].set_index(df.iloc[:5,1]+", "+df.iloc[:5,2]+", rank: "+df.iloc[:5,0].apply(str)).T,title='TOP 5 WORST Cities (Average Pollution Index 2023 by Cities)').update_layout(
    xaxis_title="Month", yaxis_title="AVG Pollution index")

px.line(df.iloc[-5:,-12:].set_index(df.iloc[-5:,1]+", "+df.iloc[-5:,2]+", rank: "+ df.iloc[-5:,0].apply(str)).T,title='Top 5 BEST Cities (Average Pollution Index 2023 by Cities)').update_layout(
    xaxis_title="Month", yaxis_title="AVG Pollution index")

"""Seems like the air pollution started to increase in the last quarter of 2023, especially those cities with high rank of pollution index.

<a id='train-test'></a>
<h2 style="font-family:arial; text-align: justify; border-bottom: 2px solid #c09f80;">Train &amp Test<a href='#contents'> ‚úÇ</a></h2>
"""

## order matter for time series data, we cannot shuffle while splitting
X = df.drop(["Rank","2023","Dec"],axis=1) ## to prevent data leak, we cannot use 2023 and Rank
y = df[["City","Country","Dec"]] ## we are going to forecast pollution level of Dec
print(f"X has {X.shape[0]} rows and {X.shape[1]} columns")
print(f"y has {y.shape[0]} rows and {y.shape[1]} columns")

X.head(3)

y.head(3)

"""<a id='preprocessing'></a>
<h2 style="font-family:arial; text-align: justify; border-bottom: 2px solid #c09f80;">Preprocessing <a href='#contents'>üîº</a></h2>
"""

def custom_preprocessing(a, category=['Country','City'], agg_func='mean'):
    a = a.groupby(category).agg(agg_func).T
    a.reset_index(names=['date'],inplace=True)
    a['date'] = pd.to_datetime(a["date"],format='%b').apply(lambda x: x.replace(year=2023))
    a.set_index(keys='date',drop=True,inplace=True)
    return a

X = custom_preprocessing(X)
y = custom_preprocessing(y)

X.head(3)

y.head()

"""<a id='model'></a>
<h2 style="font-family:arial; text-align: justify; border-bottom: 2px solid #c09f80;">Time Series Model <a href='#contents'>ü§ñ</a></h2>
In this section, we are going to employ ARIMA model to forecast the December's pollution index for every cities using their corresponding Jan-Nov data as a training set.<br>
<br>
About ARIMA<br>
One of the famous time series forecasting technique. It combines auto-regression (AR), differencing (I), and moving average (MA) components to model time-dependent data. ARIMA is widely used for forecasting stock prices, weather, and other time series patterns.

1. AutoRegressive (AR): models the relationship between a current observation and its past observations (today's pollution level vs yesterday pollution level)
2. Integrated (I): differencing the time series data to make it stationary.
3. Moving Average (MA): models the relationship between a current observation and a residual error from a moving average of past observations. It captures short-term fluctuations.
"""

def train_arima(X, y, country = None):
    result_list = []
    start_time = time.time()
    country_list = pd.Series(X.columns.get_level_values(0)).unique()

    if country == None:
        country = np.random.choice(X.columns.get_level_values(0),1)[0]

    train_df = X[country]

    for city in train_df.columns:

        model = auto_arima(train_df[city],random_state=SEED)

        y_pred = model.predict(1).values[0]

        y_true = y[country][city].values[0]

        result_list.append([country,city,y_true,y_pred,model])

    result_df = pd.DataFrame(result_list,columns=['Country','City','Dec','y_pred','model'])
    finish_time = time.time()
    print(f"Training completed in {finish_time-start_time:.2f} seconds.")

    return result_df

country = "Indonesia" # country can be replaced
result_df = train_arima(X,y,country) # if there are many cities in that country, this can take long

result_df

# show model result for a specific city
city = 'Jakarta' # replace with valid city from result_df
print(result_df[result_df["City"]==city]['model'].values[0].summary())

"""From the table, the ARIMA model for Ang Thong is AR(2) model ***(notice Model: SARIMAX(2, 0, 0))***,<br>which means the current value of a pollution index depends on its values at the two previous time periods."""

# show model result for a specific city
city = 'Jayapura' # replace with valid city from result_df
print(result_df[result_df["City"]==city]['model'].values[0].summary())

"""From the table, the ARIMA model for Chiang Rai is MA(1) model ***(notice Model: SARIMAX(0, 0, 1))***,<br>which means the current value of a pollution index depends on the previous error term (also known as the white noise term) multiplied by a coefficient."""

px.scatter(data_frame=result_df,x='Dec',y='y_pred',title=f'Fitted Vs Actual ({country})',hover_data="City").update_layout(
    xaxis_title="y_true", yaxis_title="y_pred")

"""The actual vs. predicted scatter plot shows a tight cluster of points around the diagonal line, with some deviation but not consistently above or below.<br>
It indicates that the model‚Äôs predictions are consistent with most cities.
"""

result_df['residual'] = result_df['y_pred']-result_df['Dec']
result_df['standardized_residual'] = (result_df['residual']-result_df['residual'].mean())/result_df['residual'].std()
px.scatter(data_frame=result_df, x='y_pred',y='standardized_residual',title=f'Residual Plot ({country})',hover_data=['Dec',"City"]).update_layout(
    xaxis_title="y_pred")

"""Most points are ok (gather around zero horizontal line). But noticeably higher deviations from the horizontal line on the right part, which mean the model probably faced some serious shocks/changes in the data e.g., 'Wiang Chai' at the top right.<br>
With only 11 data points per city to train on, the shock can cause the model to perform poorly.
"""

df[df["City"] == "Jakarta"].iloc[:,-12:].T.plot(kind='line') ## see wiang chai for example

"""<p style="font-size: 300%; font-family:arial; text-align: center;"><b>Thank you!!</b></p>"""